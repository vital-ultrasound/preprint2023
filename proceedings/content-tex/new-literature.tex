\section{Introduction} \label{sec:intro}
In the last decades the use of echocardiography is a crucial clinical approach in Intensive Care Units (ICU) because of the advances of smaller US clinical devices, US image quality and its real-time capabilities to access cardiac anatomy \cite{Feigenbaum1996, Vieillard-Baron2008, singh2007, cambell2018}.
However, despite the previous advances there is still challenges on finding standard views from experienced sonograpehrs that sometimes such quantifcations are qualitative and subjective \cite{Feigenbaum1996}.
Similarly, automatic quantification of left ventricular ejection fraction (LVEF) is still challenging at the point of care due to variation of protocols, skills levels \cite{field2011} and the nature of proving feedback on real-time \cite{liu2021}.

\subsection{Image Quality Assessment}
\cite{labs2021_in_miua} considers chamber clarity, depth gain, on-axis attributes, apical foreshoredness.

\subsection{Clustering techniques}
Zhang et al. mentioned that 23 view classes from 7168 individually labeled videos that ware classified with a 13-layer CNN to then viewed with the use of t-Distributed Stochastic Neighbor Embedding \cite{zhang2018}.
Kusunose et al. mentioned that other authors have reached an acciracy of 91-94 for 15-view classification while their work mentioned a 98.1 accuracy for five-prederminted views \cite{kusunose2021}.

\subsection{Auto-encoders}
Laumer et al. proposed a novel autoencoder-based framework to learn human interpretable representation of cardiac cycles from cardiac ultrasound data \cite{laumer2020},



Ouyang et al. presented echo-dynamic dataset as the first annotated medical video dataset with 10,036 videos. 
Additionally, authors reported the use of three CNN arquitectures varing filters in each layer to assess ejection fraction to near-expert performance.
It is worthwhile to note that authors got best performance with mean absolute error of 5.44\% using clip lenght of 16 and frame rate of 4.
Such error is near-expert perfonace as they can get 4-5\% for skilled echochardiographers in cotrolled settings 
\cite{ouyang-NeuripsML4H2019}.


Ghorbani et al. applied convolutional neural networks of cardiac ultrasound to identify local structures, estimate cardiac function and predict pathologies.
Their deep learning model, EchoNet, can identify up to 10 cardiac biometrics which results in decreasing repetitive task in the clinical flow, provide interpretation to less experienced cardiologist, and predict phenotipes. This work can predict age, sex, weight and heigth from echocardiogram images. 
Authors mention that the increase of date does not improve model traiining. The homogenisation of cadiac views prior to model training improved training speed and computaitonla time
\cite{Ghorbani-DigitalMedicineNature-JAN2020}


\subsection{Segmentation}
With the challenges of limited sampling of cardiac cycles and the considerable inter-observer variability, Ouyang et al. presented a CNN model with residual connections and spatiotemporal convolutions that surpase human performance of segmentaion of left ventricle, estimation of ejection fraction and assessment of cardiomyophaty. 
Their model reached Dice similarity coefficient of 0.92, predicts ejection fraction with mean absolute error of 4.1\% and clasify heart failure based on reduced ejection fraction 
%Wonder is relevant to predic and classify with our datasets in the ICU?  Fri  7 Jan 14:42:23 GMT 2022
\cite{Ouyang-Nature-APR2020}.


\subsection{Contrastive Learning}
Methods on Contrastive Learning apparently address the challenge of required labelled data to identify pathologies in the images of dectect certain cardiac views.
Recently, Chartsias et al. use contrastive learning to train imbalanced cardiac datasets and they compared a naive baseline model to achieve a F1 score of up to 26\% \cite{chartsias2021-ASMUS}
Saeed et al. recently investigated contrastive pretraining to improve the DeepLabV3 and UNET segmentation networks of cardiac structers in ultrasound imaging.
Authors showed comparable results with state-of-the-art fully supervised algorithms and presents better results compared to EchoNet-Dynamic and CAMUS \cite{saeed2021MIDL}





\subsection{AI-guided US imaging}

Near-human quantification of LV and EF has been investated, however Asch et al. pointed out that buoundary identification is prone to errors when low quality images or artifacts are used
Asch et al. pointed out that data and materials were not publicly available and they made use of AutoEF by captionhealth co.
Authors used a databes of 50000 echocardiography datasets over a period of 10 years of varios clinical US syustems. 
The training datasets included multiople views of 2 and 4-chamber views and LV EF values where clininias use conventional methods (biplane Simpson technique) \cite{asch2019CIRIMAGING}.

Asch et al. \cite{asch2021CircImaging}.

Hong et al. reported the evalition of imagin quality asssement to demostrated that AI can recognise nuaces of varing imaing during scanning \cite{hong2021JACC}


Narang et al. reported the adquisiton of 10 echocardiography views of novices users using deep-learning-based software \cite{Narang2021JAMACARDIOLOGY}.
Narang et al. mentioned that CNN were used with stacks of networks and transofrmations. 
The AI-guided software consist of three estimates: (1) quality image assement, (2) "6-dimensional geometric distance with postion and orientation between the current probe location and the locattion anticipated to optimise the image"; and (3) corrective probe manipulation. \cite{Narang2021JAMACARDIOLOGY}
Authors mention that algorithms do not use trackers, fiducial marks or additional sensors to made guide estimations \cite{Narang2021JAMACARDIOLOGY}.


Cheema et al. reported the use of AI-enabled guidance to sonoographer which was created from the use of 500000 hand movmentes.
Cheema et al.  reported that such feature was the first cardiac aotorhisedd by Food and Drug adminstation in 2020. 
Authors presented five cases covid-19 intensive care unit (ICU) to illustrate "how desition making affect in patient care" and how the use of AI-enabled provided real-time guidance to acquire desired cardiac UL with the sterting of user's transducer position and hand movevemnt \cite{CHEEMA2021JACCCaseReports}.


\subsection{Spatiotemporal Features transformers}

\subsection{Others}
Rank-2 non-negative matrix factorization \cite{yuan2017} to generate End-Systole and End-Diastole for apical 4 view.  
Recently Robust Non-negative Matrix Factorization seems to be implement low-computation cost algorithms to automatic segment mitral valve \cite{dukler2018}.



\section{Methods and materials}
