\section{Introduction} \label{sec:intro}
In the last decades the use of echocardiography is a crucial clinical approach in Intensive Care Units (ICU) because of the advances of smaller US clinical devices, US image quality and its real-time capabilities to access cardiac anatomy \cite{Feigenbaum1996, Vieillard-Baron2008, singh2007, cambell2018}.
However, despite the previous advances there is still challenges on finding standard views from experienced sonograpehrs that sometimes such quantifcations are qualitative and subjective \cite{Feigenbaum1996}.
Similarly, automatic quantification of left ventricular ejection fraction (LVEF) is still challenging at the point of care due to variation of protocols, skills levels \cite{field2011} and the nature of proving feedback on real-time \cite{liu2021}.
Studies in the management of tetanus in low- and middle-income countries (LMICs) emphasised the importance and requirement of duration of hospitalisation and mechanical ventilation requirements \cite{hao2021-wellcome}.

%TODO
%* incorporate "low- and middle-income countries (LMICs)" in the intro!
%* Read relevant work of sophie yacoub and C Louise Thwaites that can be related to our work!
    %https://scholar.google.com/citations?user=gjZ_bDkAAAAJ
    %https://scholar.google.com/citations?user=YLCVWQoAAAAJ



\section{AI-empowered methods}

\subsection{Image Quality Assessment}
\cite{labs2021_in_miua} considers chamber clarity, depth gain, on-axis attributes, apical foreshoredness.

\subsection{Clustering techniques}
Zhang et al. mentioned that 23 view classes from 7168 individually labeled videos that ware classified with a 13-layer CNN to then viewed with the use of t-Distributed Stochastic Neighbor Embedding \cite{zhang2018}.
Kusunose et al. mentioned that other authors have reached an acciracy of 91-94 for 15-view classification while their work mentioned a 98.1 accuracy for five-prederminted views \cite{kusunose2021}.

\subsection{Auto-encoders}
Laumer et al. proposed a novel autoencoder-based framework to learn human interpretable representation of cardiac cycles from cardiac ultrasound data \cite{laumer2020},
%Further reading:
%https://arxiv.org/pdf/2112.02102.pdf
%https://ieeexplore.ieee.org/abstract/document/8051114

Ouyang et al. presented echo-dynamic dataset as the first annotated medical video dataset with 10,036 videos. 
Additionally, authors reported the use of three CNN arquitectures varing filters in each layer to assess ejection fraction to near-expert performance.
It is worthwhile to note that authors got best performance with mean absolute error of 5.44\% using clip lenght of 16 and frame rate of 4.
Such error is near-expert perfonace as they can get 4-5\% for skilled echochardiographers in cotrolled settings \cite{ouyang-NeuripsML4H2019}.


Ghorbani et al. applied convolutional neural networks of cardiac ultrasound to identify local structures, estimate cardiac function and predict pathologies.
Their deep learning model, EchoNet, can identify up to 10 cardiac biometrics which results in decreasing repetitive task in the clinical flow, provide interpretation to less experienced cardiologist, and predict phenotipes. This work can predict age, sex, weight and heigth from echocardiogram images. 
Authors mention that the increase of date does not improve model traiining. The homogenisation of cadiac views prior to model training improved training speed and computaitonla time \cite{Ghorbani-DigitalMedicineNature-JAN2020}


\subsection{Segmentation}
With the challenges of limited sampling of cardiac cycles and the considerable inter-observer variability, Ouyang et al. presented a CNN model with residual connections and spatiotemporal convolutions that surpase human performance of segmentaion of left ventricle, estimation of ejection fraction and assessment of cardiomyophaty. 
Their model reached Dice similarity coefficient of 0.92, predicts ejection fraction with mean absolute error of 4.1\% and clasify heart failure based on reduced ejection fraction 
%Wonder is relevant to predic and classify with our datasets in the ICU?  Fri  7 Jan 14:42:23 GMT 2022
\cite{Ouyang-Nature-APR2020}.


\subsection{Contrastive Learning}
Methods on Contrastive Learning apparently address the challenge of required labelled data to identify pathologies in the images of dectect certain cardiac views.
Recently, Chartsias et al. use contrastive learning to train imbalanced cardiac datasets and they compared a naive baseline model to achieve a F1 score of up to 26\% \cite{chartsias2021-ASMUS}
Saeed et al. recently investigated contrastive pretraining to improve the DeepLabV3 and UNET segmentation networks of cardiac structers in ultrasound imaging.
Authors showed comparable results with state-of-the-art fully supervised algorithms and presents better results compared to EchoNet-Dynamic and CAMUS \cite{saeed2021MIDL}





\subsection{AI-guided US imaging}

Near-human quantification of LV and EF has been investated, however Asch et al. pointed out that buoundary identification is prone to errors when low quality images or artifacts are used
Asch et al. pointed out that data and materials were not publicly available and they made use of AutoEF by captionhealth co.
Authors used a databes of 50000 echocardiography datasets over a period of 10 years of varios clinical US syustems. 
The training datasets included multiople views of 2 and 4-chamber views and LV EF values where clininias use conventional methods (biplane Simpson technique) \cite{asch2019CIRIMAGING}.

Asch et al. \cite{asch2021CircImaging}.

Hong et al. reported the evalition of imagin quality asssement to demostrated that AI can recognise nuaces of varing imaing during scanning \cite{hong2021JACC}


Narang et al. reported the adquisiton of 10 echocardiography views of novices users using deep-learning-based software \cite{Narang2021JAMACARDIOLOGY}.
Narang et al. mentioned that CNN were used with stacks of networks and transofrmations. 
The AI-guided software consist of three estimates: (1) quality image assement, (2) "6-dimensional geometric distance with postion and orientation between the current probe location and the locattion anticipated to optimise the image"; and (3) corrective probe manipulation. \cite{Narang2021JAMACARDIOLOGY}
Authors mention that algorithms do not use trackers, fiducial marks or additional sensors to made guide estimations \cite{Narang2021JAMACARDIOLOGY}.


Cheema et al. reported the use of AI-enabled guidance to sonoographer which was created from the use of 500000 hand movmentes.
Cheema et al.  reported that such feature was the first cardiac aotorhisedd by Food and Drug adminstation in 2020. 
Authors presented five cases covid-19 intensive care unit (ICU) to illustrate "how desition making affect in patient care" and how the use of AI-enabled provided real-time guidance to acquire desired cardiac UL with the sterting of user's transducer position and hand movevemnt \cite{CHEEMA2021JACCCaseReports}.



\subsection{Annotation tools}
Recently, Smistad et al. 2021 published the first web-based tool for annoration of medical ultrasound video to do image classification, segmentation, bouding box and landmark annotation  \cite{smistad2021-A-IUS}. 
AW tool has been used since 2016 at different projects to perform segmentation of the left ventricle, cardiac view classification, and detection of nerves and blood vesels
\cite{smistad2021-A-IUS}.

\subsection{3D US}
Considering that 3D left ventricle (LV) can provide full volume information of the hearth than 2D echordaiography,  Dong et al. proposed a real-time framework VoxelAtlasGAN that made use of cGAN \cite{dong2018-MICCAI}. 
VoxelAtlasGAN framework with mean surface distance of 1.85 mm, mean hausdorff distance of 7.66mm, mean dice 0.953 and correlation of EF 0.918 and the mean inference speed of 0.1 s demostrated potential for clinical applicaiton \cite{dong2018-MICCAI}.
Dong et al. in 2020 applied tranformers to obtain translations parematers that passed to VoxelAtlasGAN \cite{dong2020-MIA}.
AlasNET framework  ended up with "mean surface distance, mean hausdorff surface distance, and mean dice index were 1.52 mm, 5.6 mm and 0.97 respectively" \cite{dong2020-MIA}

Smistad et al. 2021 made use of CETUS 3D US LV segmentation dataset and weakly annorated datasets for real-time 3D left ventricle segmentation and estiomation of ejection fraction \cite{smistad2021-D-IUS}.
Authors presented the impact of pre-training that resulted in an improvement of Dice score. 
It is imporant to note that VoxelAlasGAN and AtlasNet by Dong el al. presented a better dice score.
Smistad et al. 2021 concluded that a limited labelled datasets of 15 patiens demostrate good accuracy and models were able to generalise to new data and ultrasoudn scanners \cite{smistad2021-D-IUS}.




\subsection{Transformers}

Rubin et al. noted the shortcoming of transformers of extensive compation for training that lead to use detection transformer (DETR) which make smaller models reducing model size and acceleration inference \cite{rubin2021-PMLH}.,
Rubin et al. considered the detection of needles in real-time ultrasound video sequences 12,000 needle insertions (2 million of individual frames).
Video sequences (up to 60 sec in time) were divided into 30-frame clips (1 sec in time).


Reynaud et al. 2021 adapted Residual Autoencoder Network and BERT model to predict ejection fraction which is different from what is commonly use with segmentation methods \cite{reynaud2021-MICCAI}.
Reynaud et al. applied their model to Echonet-Dynamic dataset which only constains 10,030 echocardiograms containing one to three or more cadiac cycles with only cardiac cycle with ES and ED annorations.
Due to the distribution between ES and ED, the sequence legnth was 128 frames.
As Echonet-Dynamic datasets contains unlabelled ES and ED, Reynaud et al. applied (a) Guided Random Sampling (b) Mirroring Methods.
Code is available at \url{https://github.com/HReynaud/UVT}.



\subsection{Others}
Rank-2 non-negative matrix factorization \cite{yuan2017} to generate End-Systole and End-Diastole for apical 4 view.  
Recently Robust Non-negative Matrix Factorization seems to be implement low-computation cost algorithms to automatic segment mitral valve \cite{dukler2018}.

Salte et al. classified three standard appical views from data of 200 patients to peformf straing measurements with deep learning arquitectures \cite{SALTE2021-JACC}.
Salte el al. made use of the work \cite{ostvik2021-TMI} inception and dense network were used to clasity, recurrent network to detect event timing and u-net-based network for segmentation \cite{SALTE2021-JACC}.
Authors compared the results with the commenrially avaible semiautomatic speckle-tracking software (EchoPAC v202), reporting evinde of the comparable GLS measurmentes to other semiatuomatic methods \cite{SALTE2021-JACC}.




\section{Spatiotemporal Features}


\subsection{Deep Residual Learning}
Ouyang et. al. benmarked various spatiotemporal convolutions (Sports-1M, Kinetics, UCF101, and HMDB51) \cite{ouyang-NeuripsML4H2019}  based on deep residual learning (He et al. 2015 and Tran et al. 2018).

%"In the interest of creating high performance baselines on standard videos with gray-scale images without
%proprocessing or creation of optical flow frames, we evaluated the performance of three architectures
%which combine 3D convolutions over spatiotemporal video volume with residual connections between
%layers[32]. Each model uses ResNet-18 as the base architecture and consists of 18 convolutional
%layers with residual connections connecting odd numbered layers [16].

%32:  
%A Closer Look at Spatiotemporal Convolutions for Action Recognition
%single type of network (ResNet) and a homogenous use of our (2+1)D spatiotemporal decomposition
%https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html32: 
%https://arxiv.org/pdf/1711.11248.pdf

%16:
%Deep Residual Learning for Image Recognition 
%He et al. 2015
%https://arxiv.org/abs/1512.03385


\subsection{LSTM}
Recently, Smistad et al. 2021 presented the use of LSTM to address the single frame segmentation of end-diastole and end-systole to address segmentation fricklering and reduce tmeporal errors \cite{smistad2021-C-IUS}.
One of the challenges is architecture design to add ConvLSTMs to whcih authors experiment at the location at the endocer, decoder, last layer and in bottlenet, to which authors mentiosn that the use of the ConvLSTM layers in the encoder of the temporal NN gave the best results \cite{smistad2021-C-IUS}.
Authors mention that interpolation of the annorations of the entire cardiac cycle did not captured the complex motion witht he use of 7 frames to which they suggest to use advance speckle tracking such as Echo-PWC-Net  \cite{ostvik2021-TMI}.

Lu et al. made use of U-Net and LSTM to model Left Ventricular cardiac motion \cite{lu2020-MIUA}.




\section{Open datasets}

\subsection{2D echochardiography}
\subsubsection{CAMUS}
CAMUS dataset, Cardiac Acquisitions for Multi-structure Ultrasound Segmentation, was publised in 2019 by Leclerc et al. 2019 \cite{leclerc2019-IEEETransMedicalImaging}.
CAMUS is the largest publicy-available and fully-annotated dataset of two and four-chamber adquistion from 500 patients.
Datasets is cathegorised in image quality (good, medium, and poor) and $LV_{EF}$ ($\leq$ 45\% (phatological risk) , $\geq$ 55\%, else).
The dataset reflects a daily clinical practice data where images quality and a range of phatological cases.
Dataset was collected with GE Vivid E95 ultrasound scanners (GE Vingmed Ultrasound, Horten Norway) with a GE M5S probe (GE Healthcare, US).
The datasets is available electronically to download at \url{https://www.creatis.insa-lyon.fr/Challenge/camus/}.
%Non-deep-learning techniques analyse cardiac cycles to analysis end-diastole and end-systolic left ventricular volumes with a mean correlation of 0.95 and absolute mean error of 9.5 ml.

\subsubsection{EchoNet-Dynamic}
Ouyang et al. published a large datasets of 10,030 annotated echocardiogram videos \cite{ouyang-NeuripsML4H2019, Ouyang-Nature-APR2020}.
Datasets were labelled left ventricle volumes by sonographers to calculate ejection fraction.
Datasets were acquired by skilled sonographers using iE33, Sonos, Acuson SC2000, Epiq 5G or Epiq 7C ultrasound machines and processed images were stored in a Philips Xcelera system.
The datasets is available electronically to download at \url{https://echonet.github.io/dynamic/index.html#dataset}.
%he resulting square images wereeither 600 × 600 or 768 × 768 pixels depending on the ultrasoundmachine and down sampled by cubic interpolation using OpenCVinto standardized 112 × 112 pixel videos.

\subsection{3D echochardiography}
\subsubsection{CETUS}
CETUS datset, Challenge on Endocardial Three-dimensional Ultrasound Segmentation, was published in 2016 by Bernard et al. \cite{bernard2016TMI}.
CETUS contains 45 sequences of 3D ultrasound volumes of one cardiac cycle from 45 patients were equally acquired from three different hospitals with three different brands of ultrasound machines (GE, Philips and Siemens) \cite{bernard2016TMI}.
The studied population of 45 participants is composed of 15 healthy subjects, 15 with previous myocardial infartation, 15 with dilated cardiography.
The datasets is available electronically to download at \url{https://www.creatis.insa-lyon.fr/EvaluationPlatform/CETUS/about_database.html}.

\section{Methods and materials}

\section{Datasets}

\subsection{VITAL}
86 patients of average age (?) ? male and ? female were collected by four clinicians of ? years of expience collected echochardiography datasets.
The collection was done with the clinical device GE Venue Go machine and GE convex probe C1-5-D.

\subsection{Ethics statement}
This study was approved by ... and the ethics committee ...
All participants gave written informed consent to participate before enrollment.