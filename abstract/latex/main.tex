%%%% SELECT ONE OF THE FOLLOWING COMMANDS %%%%%%%%

%%% TEMPLATE FOR PROCEEDINGS TRACK %%%%
%\documentclass[mlmain,twocolumn]{jmlr}

%% TEMPLATE FOR Extended Abstract Track %%%%%%%
\documentclass[mlabstract,twocolumn]{jmlr}
%\documentclass[mlabstract]{jmlr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
% Watermark
%These 4 commands must be removed for the camera-ready version.
\usepackage[hpos=300px,vpos=70px]{draftwatermark}
\SetWatermarkText{\test}
\SetWatermarkScale{1}
\SetWatermarkAngle{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%


%%OVERLEAF
%% Anyone with this link can edit this project
%% https://www.overleaf.com/4969677577xfsfbjqkrnfx


% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e


%%% WARNING %%%%
%%% 1) Please, use the packages automatically loaded to manage references, write equations, and include figures and algorithms. The use of different packages could create problems in the generation of the camera-ready version. Please, follow the examples provided in this file.
%%% 2) References must be included in a .bib file.
%%% 3) Write your paper in a single .tex file.
%%%

%%%% SOFTWARE %%%%
%%% Many papers have associated code provided. If that is your case, include a link to the code in the paper as usual and provide a link to the code in the following comment too. We will use the link in the next comment when we generate the proceedings.
%%% Link to code: http://?? (only for camera ready)

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

 % Define an unnumbered theorem just for this sample document:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

%%%% DON'T CHANGE %%%%%%%%%
\jmlrvolume{}
\firstpageno{1}
\editors{List of editors' names}

\jmlryear{2022}
\jmlrworkshop{Machine Learning for Health (ML4H) 2022}

%\editor{Editor's name}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\title[Short Title]{
%  Full Title of Article\titlebreak This Title Has A Line Break\titletag{\thanks{sample footnote}}
%Real-time AI-empowered echocardiography for Intensive Care Units in low- and middle-income countries %Mon 20 Jun 11:48:09 BST 2022
%Challenges in Real-time AI-empowered echocardiography for Intensive Care Units in low- and middle-income countries. %Fri 19 Aug 15:36:45 BST 2022
Challenges in Real-time AI-empowered echocardiography for Intensive Care Units in low- and middle-income countries: A Machine Learning Case Study %Fri 19 Aug 16:43:05 BST 2022
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE MANUSCRIPT, DATA AND CODE MUST BE ANONYMIZED DURING THE REVIEW PROCESS.
% DON'T INCLUDE ANY INFORMATION ABOUT AUTHORS DURING THE REVIEW PROCESS.
% Information about authors (Full names, emails, affiliations) have to be provided only for the submission of the camera-ready version.  Only in that case, you can uncomment and use the next blocks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

   \author{
     \Name{Anonymous Author(s)} \Email{email@sample.com}
      \addr Address
   }

 % Use \Name{Author Name} to specify the name.

 % Spaces are used to separate forenames from the surname so that
 % the surnames can be picked up for the page header and copyright footer.

 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % *** Make sure there's no spurious space before \nametag ***

 % Two authors with the same address
%   \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}

  %Three or more authors with the same address:
%   \author{\Name{Author Name1} \Email{an1@sample.com}\\
%   \Name{Author Name2} \Email{an2@sample.com}\\
%   \Name{Author Name3} \Email{an3@sample.com}\\
%   \Name{Author Name4} \Email{an4@sample.com}\\
%   \Name{Author Name5} \Email{an5@sample.com}\\
%   \Name{Author Name6} \Email{an6@sample.com}\\
%   \Name{Author Name7} \Email{an7@sample.com}\\
%   \Name{Author Name8} \Email{an8@sample.com}\\
%   \Name{Author Name9} \Email{an9@sample.com}\\
%   \Name{Author Name10} \Email{an10@sample.com}\\
%   \Name{Author Name11} \Email{an11@sample.com}\\
%   \Name{Author Name12} \Email{an12@sample.com}\\
%   \Name{Author Name13} \Email{an13@sample.com}\\
%   \Name{Author Name14} \Email{an14@sample.com}\\
%   \addr Address}


%%  Authors with different addresses:
%  \author{\Name{Author Name1} \Email{abc@sample.com}\\
%  \addr Address 1
%  \AND
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address 2
% }



\begin{document}

\maketitle

\begin{abstract}
We present a machine learning case study presenting the challenges of implementing real-time AI-empowered echocardiography systems in the ICU in LMICs.
We present results from a small video dataset of 31 subjects, data preparation, curation and labelling, code implementation and model selection, validation and deployment.
The code and other resources to reproduce this work are available at \url{https://github.com/vital-ultrasound/echocardiography}.
\end{abstract}
\begin{keywords}
echocardiography; artificial intelligence
\end{keywords}

\section{Introduction}
\label{sec:intro}
Echocardiography is an important clinical procedure in Intensive Care Units (ICU) because of the advances of Ultrasound (US) such as portability, low cost, low radiation and its real-time capabilities to access cardiac anatomy \citep{Feigenbaum1996, Vieillard-Baron2008, singh2007, cambell2018}.
Despite that, there various challenges in the current clinical procedures in the ICU:
\begin{itemize}
\setlength\itemsep{0em}
\item Intra-view variability of echocardiograms (physiological variations of subjects and acquisition parameters) and sonographer expertise \citep{khamis2017, Feigenbaum1996, field2011},
\item Inter-view similarity of echocardiograms (similar views of valve motion, wall motion, left ventricle, etc) and transducer position during acquisition \citep{zhang2018},
\item Redundant information in the clinical echo system (icons, date, frame rate, etc) \citep{khamis2017} and variation of Ultrasound images from different clinical US systems \citep{brindise2020unsupervised}, and
\item Limited number of expert clinicians to perform US imaging analysis and to provide accurate diagnosis, as well as equipment and hospitalisation requirements in low- and middle-income countries (LMICs) \citep{hao2021-wellcome, 2021-huyNhat-vanHao-in-FAIR-MICCAI}.
\end{itemize}
One promising approach to address such challenges is with the application of Artificial Intelligence to echocardiography, AI-empowered echocardiography, which has been successful to detect different apical views, inter-observer variability of sonographer's expertise, one-stop AI models with multimodal imaging (US, MRI and clinical data), high risk or low risk of heart failure detection or automatic endocardial border detection and left ventricle assessment in 2D echocardiography videos \citep{tromp2022, zhang2022-mdpi, behnami2020, ono2022}.
However, there is little to none studies on how real-time AI-empowered echocardiography might impact the ICU in LMICs.
Particularly, how good machine learning practices (data curation, code implementation, model selection, training and tuning; model validation and inference) are followed to address the challenges on real-time AI-empowered echocardiography in the ICU in LMICs.
Hence, this work presents (a) a scoping review of AI-empowered echocardiography in the ICU and (b) real-time AI-empowered echocardiography, (c) a machine learning case of study of US image classification using deep learning of four chamber views from curated data from LMICs and (d) conclusions future work and appendix with further material.

% \subsection{AI-empowered echocardiography}
% Tromp et al. classified a dataset of 1145 2D echocardiography videos as apical 4 chamber (A4C) view, apical 2 chamber (A2C) view, parasternal long axis (PLAX) view, or 2D other views and focused versions of the main views \citep{tromp2022}.
% Authors used CNN of four layers, dense network and softmax output layer, trained with categorical cross-entropy loss function, then a second classifier of an unsupervised deep learning clustering CNN, trained with mean square error and Kullback-Leibler loss functions \citep{tromp2022}.
% %For the remaining dataset (2,126 PSAX-PM echo cines), while the LV segmentation is not available, the ground truth LVEF values are acquired from the patientsâ€™ archived information.

% \citet{zhang2022-mdpi}
% reviewed AI's applications in left ventricular systolic function (LVEF) and global longitudinal strain (GLS), pointing out its dependency to the sonographers's expertise (inter-observer variability) and post-processing and variability in different US devices.
% \citet{zhang2022-mdpi} pointed the challenges of AI-enhanced echocargiografy for interpretability of results and its sensitivity to sample shortage, to which authors mention about the potentials of multimodal imaging (us, mri and clinical data) to improve detection rate of diseases.

% \citet{behnami2020} applied DenseNet-like network for feature learning and RNN unit with bidirectional Gated Recurrent Units to alleviate loss of information from the earlier frames of echos to automatically detect high risk or low risk of heart failure with reduced ejection fraction with an overall accuracy of 83.15\%, precision of 82.6\% and recall of 81.1\%.
% \citet{behnami2020} mentioned that EF is highly user-dependant to which they propose to collect more data,

% \citet{liu2021JMIA} proposed pyramid local attention neural network (PLANet) to improve segmentation performance of automatic methods in 2D echocardiography.
% PLANet was evaluated with CAMUS and sub-EchoNet-Dynamic datasets, showing a better performance against geometric and clinical metrics.

% \citet{ulloaCerna2021} made use of DNN to learn spatiotemporal features from echocardiography video data to enhance clinical prediction of 1 yr all-cause mortality where video echo data linked to EHR data that included hand-crafted echocardiography-derived measurements (EDMs), additional clinical variables and individual outcomes.
% The DNN model presents "superior prediction performance" over four cardiologist and two benchmark clinical models: the pooled cohort equations (PCE) and Seattle Heart Failure (SHF) risk score \citep{ulloaCerna2021}.
% \citet{ulloaCerna2021} used "full, raw (annotation-free) echocardiographic videos to make predictions by learning from more than 812,278 clinically acquired echocardiography videos of the heart (50 million images)."

% \citet{jafari2021} pointed out the challenges of obtained high quality for less experience operators and the hight variability or echo quality adn cardiovascular structures across different patients to which authors proposed "Bayesian deep learning approach for fully automatic LVEF estimation based on segmentation of the left ventricle (LV) in parasternal short-axis papillary muscles (PSAX-PM) level".
% \citet{jafari2021} made use of 2,680 patients with PSAX-PM echo cine acquired by a variety of ultrasound devices, namely iE33, Vivid i/7/9/95, Sonosite, and Sequoia (only 554 echo cines were considered as ground truth with LV mask delineated by an experienced level III echocardiographer).

% Ono et al. applied different models where Unet++ demonstrated good performance for automatic endocardial border detection and left ventrical assessment in 2D echocardiography videos \citep{ono2022}.
% The datasets to train networks was made of 2798 images from 118 videos of which 22 videos with 465 frames were for 4CV \citep{ono2022}.
% Ono et al. also touched on the challenges of providing explainable AI for US imaging.


\section{AI-empowered echocardiography in the ICU}
\citet{CHEEMA2021JACCCaseReports} presented five cases covid-19 intensive care unit (ICU) to illustrate "how decision making affect in patient care" and how the use of AI-enabled provided real-time guidance to acquire desired cardiac UL with the sterting of user's transducer position and hand movement.
\citet{hanson2001} reviewed various applications of AI in the ICU where real-time analysis of waveforms of electrocardiograms and electroencephalograms using neural network were used to identify cardiac ischemia and diagnosis myocardial ischemia.
\citet{hanson2001} also reviewed various scenarios where AI is used in the ICU, such as Bayesian networks considering central venous pressure (CVP), left ventricular ejection fraction (EF), heart rate (HR), hemoglobin (HGB) and oxygen saturation (O2sat) resulting in a probabilistic cardiac output.
\citet{hanson2001} also touched on data visualisation to demonstrate the hypothetical ICU for large number of patients (head injury, sepsis, acute respiratory distress syndrome, etc).
\citet{Ghorbani-DigitalMedicineNature-JAN2020} reported the first deep learning model to predict age, sex, weight and height from echocardiogram images and make use of such models to understand how models predicts systematic phenotypes which are difficult for human interpreters.
Authors trained CCN models with 2.6 million echocardiogram images from 2850 patients with the extraction of labels local structures and features (e.g. pacemaker lead, dilation of left atrium, hypertrophy for left ventricular) and labels from the physician-interpreted report (e.g, catheters, pacemaker, and defibrillator leads).
Recently, \citet{hong2022} reviewed 673 papers that made use of machine learning-enabled to help for clinical decision in the ICU, of these studies the majority used supervised learning (91\%) few doing unsupervised learning and reinforcement learning.
Similarly, \citet{hong2022} identified 20 of the most frequent variables in machine learning-enabled in the ICU, being the top five (age, sex, heart rate, respiratory rate, and pH).
\citet{hong2022} mentioned that typical outcomes in the ICU are mortality, survival, and long-term quality of life and included typical patient outcomes, specific diseases, and stay of time evaluation.
%For specific diseases, the most studied are sepsis, infection and kidney injury; and trends with liver diseases and severe cancerl and others like cardica diseases, brain diseases;
%\citep{2021-huyNhat-kerdegari-in-FAIR-MICCAI}

\section{Real-time AI-empowwered echocardiography}
%In terms of real-time analysis of echocardigraphy, , 
\citet{wu2022} applied baselines of UNET with temporal context-aware encoder (TCE) and bidirectional spatiotemporal semantics fusion (BSSF) modules to EchoDynamic (10030 video sequences with of 200frames of 112x112 pixes) and CAMUS datasets  (450 video of 20 frames of 778x594 pixels) with evaluation metrics of Dice score (DS), Hausdorff Distance (HD), and area under the curve (AUC).
\citet{wu2022} presented speed analysis, ensuring low latency and real-time performance, against eight methods using calculations number FLOPS (G), number of parameters (M) and speed ($ms/f$) which lowest one was 32 $ms/f$.
\citet{woudenberg2018} trained an DenseNet-LSTM with 2K clips of 4 chamber view in which the real-time system made use of 10 input frames and reported a latency of 352.91ms.
\citet{toussaint2018-MICCAI} reported ResNet18-SP trained with 85k frames of Fetal US imaging with real-time performance of $\sim$20Hz.
\citet{ostvik2021-TMI} proposed Echo-PWC-Net trained with Synthetic/Simulated/Clinical  for real-time using 7 frames for the input.
\begin{figure}[htbp]
\floatconts
  {fig:main-figure}
  {\caption{Real-time AI clinical system (a) clinical system, (b) deep learning pipeline.}}
  {\includegraphics[width=\columnwidth]{../figures/main-figure/versions/drawing-v00}}%%GITHUB
    % {\includegraphics[width=\columnwidth]{figures/main-figure.png}}%%OVERLEAF
    %{\includegraphics[width=\columnwidth]{fig01.png}} %%ARXIV
\end{figure}

\subsection{Classification of echochardiograms}
\citet{woudenberg2018} applied DenseNet and LSTM to extract temporal information on sequences of 16K echo cine frames to classify 14 heart views with an average accuracy of 92.35\%.
\citet{woudenberg2018} implemented a Tensorflow runner that performs contrast enhancement to then sent each frame to three identical CNNs running in separated threads to prevent lag during inference times.
Then a shared buffer collects extracted features from CNNs to then awake the thread for the LSTM network from the previous ten frames to produce classification and quality prediction.
\citet{woudenberg2018} also presents timing diagrams to quantify frame arrival and real-time performance to operate at 30 frames per second, while providing feedback with a mean latency of 352.91 Â± 38.27 ms when measured from the middle of the ten-frame sequence.
\citet{zhang2018} performed view classification with 277 echocardiograms to create a 23-class models (including a4c no occlusions, a4c occluded LA, a4c occluded LV, etc) using 13-layer CNN with 5-fold cross-validation for accuracy assessment and resulting in 84\% for overall accuracy where challenges for partial obscured LVs for a2c, a3c and a4c.
Similarly, Zhang et al. applied U-net to segment 5 views (a2c, a3c, a4c, PSAX, PLAX) and CNN model for 3 cardiac diseases with the use of A4c capturing most of the information for the diseases.
\citet{khamis2017} considered 309 clinical echocardiogram of apical views which were visually classified and labelled by two experts into three classes: 103 a2c views, 103 a4c views and 103 alx views to then applied spatio-temporal feature extraction (Cuboic Detector) and supervised learning dictionary (LC-KSVD) resulting in an overall recognition rate of 95\%.

\subsection{Light neural networks to classify US images}
\citet{baumgartner2017-IEEETransMedImag} proposed SonoNet which is a VGG-based architecture, SonoNet64 used the same first 13 layers of VGG16,
and SmallNet, loosely inspired by AlexNet, for real-time detection and bounding box localisation of standard views in freehand fetal US.
\citet{toussaint2018-MICCAI}
applied four feature extraction networks couple with batchnormalization and soft proposal layer (VGG13-SP, VGG16-SP, ResNet18-SP, ResNet34-SP) being ResNet18-SP the best performing network with average accuracy over six classes of fetal US views (0.912).
Authors mentions that detection and localisation of anatomical views were tested in real-time performance at inference time (40ms per image, or ~20Hz).
\citet{Al-Dhabyani2019-IJACSA} applied AlexNet and transfer learning based architectures (VGG16, Inception, ResNet, NASNet) without augmentation and with three augmentation techniques to perform tumor classification of breast ultrasound imaging. Authors stated that transfer learning NASNet presented the best performance with 99\% with BUSI+B datasets with DAGAN augmentation.
\citet{xie2020-physics-in-medicine-biology} proposed a dual-sampling convolutional neural network (DSCNN) for US image breast cancer classification, being DSCNN more efficient than AlexNet, VGG16, ResNet18, GoogleNet and EfficientNet.
\citet{snider2022-ScientificReports} reported summaries of CNN heuristics to detect shrapnel in US images.
Authors presented summaries of model performance for layer activators, 2D CNN layer architectures, model optimisers dense nodes, and the effect of image augmentation and dropout rate and epoch number.
\citet{boice2022-in-jimaging} proposed ShrapML, a CNN model to detect shrapnel in US imaging.
Authors compared ShrapML (8layers--6CNN,2FC, 0.43 million of parameters) against DarkNet19, GoogleNet, MobileNetv2 and SqueezeNet, being ShrapML 10x faster than MobileNet2 which offered the highest accuracy.

\begin{table}[htbp]
\floatconts
  {tab:example}%
  {\caption{Neural Networks}}%
{\begin{tabular}{lllll}
\textbf{Networks} & Parameters & Source \\
MobileNetV1 & 3,208,450 &   \\ 
MobileNetV2 & 2,225,858 &    \\
\citet{iandola2017squeezenet} & 733,580 &    
\end{tabular}}
\end{table}

\subsection{Datasets}
Echocardiography videos of 31 subjects in the ICU were considered for this which were collected by four clinicians of ? years of experience collected using clinical device GE Venue Go machine and GE convex probe C1-5-D.
The 31 subjects has the following characteristics:
Sex: \% (Male): 58.1\%;
Age: mean, years (std): 38.70 (16.08);
Weight: mean, Kg (std): 61.51 (15.06);
Height: mean, m (std): 1.62 (0.07);
BMI: mean (std): 23.80 (4.30);
Sepsis \% (with): 61.3\%;
Dengue \% (with): 54.8\%, and
Tetanus \% (with): 87.1\%.
See \figureref{fig:demographics} with further details on the demographics of the dataset, including the complete dataset of 87 subjects.

\subsubsection{Ethics statement}
This study was approved by ? and the ethics committee ?
All participants gave written informed consent to participate before enrollment.

\section{Heuristics}
\figureref{fig:heuristics} shows validation loss curves against three models (MobileNetV1, MobileNetV2, SqueezeNet).
See \figureref{fig:SqueezeNetTrainingResults} for further results on SqueezeNet Training performance.  

\begin{figure}[htbp]
\floatconts
  {fig:heuristics}
  {\caption{heuristics.}}
  {\includegraphics[width=\columnwidth]{../figures/heuristics/versions/drawing-v00}}%%GITHUB
    % {\includegraphics[width=\columnwidth]{figures/heuristics.png}}%%OVERLEAF
    %{\includegraphics[width=\columnwidth]{fig01.png}} %%ARXIV
\end{figure}

\section{Conclusions and Future Work}
2D velocity vector fields of flow blow can help to detect abnormal flow patterns as done in fetal and neonatal echocardiography \citep{Meyers2020}.
%Use LV A4C echos that can
Create synthetic Ultrasound images for GE Vivid E9, Hitachi Prosound U7, Philips iE 33 Vision, Siemens SC2000, and Toshiba Artida ultrasound systems \citep{brindise2020unsupervised}.




\acks{Acknowledgements go here.}

\bibliography{../references/references}%%%GITHUB
% \bibliography{references}%%%OVERLEAF
%\input{main.bbl}%%%ARXIV

\appendix

\section{First Appendix}\label{apd:first}
This is the first appendix.

\begin{figure}[htbp]
\floatconts
  {fig:SqueezeNetTrainingResults}
  {\caption{Heuristics for 5 and 33 subjects with 10 frames per clip and 25 batch size of clips using SqueezeNet \citep{iandola2017squeezenet}.}}
  {\includegraphics[width=\columnwidth]{../figures/squeezenet-05-33-subjects/versions/drawing-v00}}%%GITHUB
    % {\includegraphics[width=\columnwidth]{figures/squeezenet-05-33-subjects.png}}%%OVERLEAF
    %{\includegraphics[width=\columnwidth]{fig01.png}} %%ARXIV
\end{figure}


\section{Second Appendix}\label{apd:second}
This is the second appendix.

\begin{figure}[htbp]
\floatconts
  {fig:demographics}
  {\caption{Patient demographics.}} %Figure is adapted from the works of %\cite{}.
  {\includegraphics[width=\columnwidth]{../figures/patient-demographics-and-diseases/versions/drawing-v01}}%%GITHUB
    % {\includegraphics[width=\columnwidth]{figures/patient-demographics-and-diseases.png}}%%OVERLEAF
    %{\includegraphics[width=\columnwidth]{fig01.png}} %%ARXIV
\end{figure}


\end{document}
