# 
## Citations


1
https://scholar.google.com/scholar?cites=16826618304834468852&as_sdt=2005&sciodt=0,5&hl=en
Retrived
Sun 21 Aug 21:35:17 BST 2022


## Authors 

## Notes

## Links 

## Bibtex 

```


@Article{app112411697,
AUTHOR = {Kerdegari, Hamideh and Phung, Nhat Tran Huy and McBride, Angela and Pisani, Luigi and Nguyen, Hao Van and Duong, Thuy Bich and Razavi, Reza and Thwaites, Louise and Yacoub, Sophie and Gomez, Alberto and VITAL Consortium},
TITLE = {B-Line Detection and Localization in Lung Ultrasound Videos Using Spatiotemporal Attention},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {11697},
URL = {https://www.mdpi.com/2076-3417/11/24/11697},
ISSN = {2076-3417},
ABSTRACT = {The presence of B-line artefacts, the main artefact reflecting lung abnormalities in dengue patients, is often assessed using lung ultrasound (LUS) imaging. Inspired by human visual attention that enables us to process videos efficiently by paying attention to where and when it is required, we propose a spatiotemporal attention mechanism for B-line detection in LUS videos. The spatial attention allows the model to focus on the most task relevant parts of the image by learning a saliency map. The temporal attention generates an attention score for each attended frame to identify the most relevant frames from an input video. Our model not only identifies videos where B-lines show, but also localizes, within those videos, B-line related features both spatially and temporally, despite being trained in a weakly-supervised manner. We evaluate our approach on a LUS video dataset collected from severe dengue patients in a resource-limited hospital, assessing the B-line detection rate and the model&rsquo;s ability to localize discriminative B-line regions spatially and B-line frames temporally. Experimental results demonstrate the efficacy of our approach for classifying B-line videos with an F1 score of up to 83.2% and localizing the most salient B-line regions both spatially and temporally with a correlation coefficient of 0.67 and an IoU of 69.7%, respectively.},
DOI = {10.3390/app112411697}
}


```

