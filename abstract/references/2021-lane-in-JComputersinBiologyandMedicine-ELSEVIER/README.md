# 
## Citations
1
https://scholar.google.com/scholar?cites=3582768052701840503&as_sdt=2005&sciodt=0,5&hl=en

Retrived
Tue 12 Oct 16:01:05 BST 2021

## Authors 

https://scholar.google.com/citations?hl=en&user=vDOP31sAAAAJ&view_op=list_works&sortby=pubdate



## Notes

## Links 
https://www.sciencedirect.com/science/article/abs/pii/S0010482521001670

https://intsav.github.io/phase_detection.html

https://github.com/intsav/EchoPhaseDetection




## Bibtex 

```
@article{LANE2021104373,
title = {Multibeat echocardiographic phase detection using deep neural networks},
journal = {Computers in Biology and Medicine},
volume = {133},
pages = {104373},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104373},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001670},
author = {Elisabeth S. Lane and Neda Azarmehr and Jevgeni Jevsikov and James P. Howard and Matthew J. Shun-shin and Graham D. Cole and Darrel P. Francis and Massoud Zolgharni},
keywords = {Echocardiography, Cardiac imaging, Deep learning, Phase detection},
abstract = {Background
Accurate identification of end-diastolic and end-systolic frames in echocardiographic cine loops is important, yet challenging, for human experts. Manual frame selection is subject to uncertainty, affecting crucial clinical measurements, such as myocardial strain. Therefore, the ability to automatically detect frames of interest is highly desirable.
Methods
We have developed deep neural networks, trained and tested on multi-centre patient data, for the accurate identification of end-diastolic and end-systolic frames in apical four-chamber 2D multibeat cine loop recordings of arbitrary length. Seven experienced cardiologist experts independently labelled the frames of interest, thereby providing infallible annotations, allowing for observer variability measurements.
Results
When compared with the ground-truth, our model shows an average frame difference of −0.09 ± 1.10 and 0.11 ± 1.29 frames for end-diastolic and end-systolic frames, respectively. When applied to patient datasets from a different clinical site, to which the model was blind during its development, average frame differences of −1.34 ± 3.27 and −0.31 ± 3.37 frames were obtained for both frames of interest. All detection errors fall within the range of inter-observer variability: [-0.87, −5.51]±[2.29, 4.26] and [-0.97, −3.46]±[3.67, 4.68] for ED and ES events, respectively.
Conclusions
The proposed automated model can identify multiple end-systolic and end-diastolic frames in echocardiographic videos of arbitrary length with performance indistinguishable from that of human experts, but with significantly shorter processing time.}
}
```

